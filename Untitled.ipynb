{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4314785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import pprint\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import zipfile\n",
    "import syllables\n",
    "import string\n",
    "import re\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15bb3be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL_ID                                                URL\n",
       "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...\n",
       "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...\n",
       "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...\n",
       "3  bctech2014  https://insights.blackcoffer.com/effective-man...\n",
       "4  bctech2015  https://insights.blackcoffer.com/streamlined-t..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Attempt to read the Excel file\n",
    "file_path = 'Input.xlsx'\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "    print(\"File read successfully!\")\n",
    "except UnicodeDecodeError:\n",
    "    print(\"UnicodeDecodeError: trying to read with a different encoding.\")\n",
    "    df = pd.read_excel(file_path, encoding='utf-8')  # Try another encoding if needed\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the url file into the pandas object\n",
    "df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "#loop throgh each row in the df\n",
    "for index, row in df.iterrows():\n",
    "  url = row['URL']\n",
    "  url_id = row['URL_ID']\n",
    "\n",
    "  # make a request to url\n",
    "  header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
    "  try:\n",
    "    response = requests.get(url,headers=header)\n",
    "  except:\n",
    "    print(\"can't get response of {}\".format(url_id))\n",
    "\n",
    "  #create a beautifulsoup object\n",
    "  try:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  except:\n",
    "    print(\"can't get page of {}\".format(url_id))\n",
    "  #find title\n",
    "  try:\n",
    "    title = soup.find('h1').get_text()\n",
    "  except:\n",
    "    print(\"can't get title of {}\".format(url_id))\n",
    "    continue\n",
    "  #find text\n",
    "  article = \"\"\n",
    "  try:\n",
    "    for p in soup.find_all('p'):\n",
    "      article += p.get_text()\n",
    "  except:\n",
    "    print(\"can't get text of {}\".format(url_id))\n",
    "\n",
    "  #write title and text to the file\n",
    "  file_name = '' + str(url_id) + '.txt'\n",
    "  with open(file_name, 'w') as file:\n",
    "    file.write(title + '\\n' + article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ccb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_folder = r\"StopWords\"\n",
    "\n",
    "# Initialize an empty set to store the stop words\n",
    "stop_words = set()\n",
    "\n",
    "# Iterate through each file in the stop words folder\n",
    "for file_name in os.listdir(stop_words_folder):\n",
    "    file_path = os.path.join(stop_words_folder, file_name)\n",
    "    \n",
    "    # Open and read each stop words file\n",
    "    with open(file_path, 'r', encoding='latin-1') as word_file:\n",
    "        word_list = [line.strip().lower() for line in word_file]\n",
    "        stop_words.update(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109c5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words_file = r\"positive-words.txt\"\n",
    "with open(positive_words_file, 'r', encoding='latin-1') as pos_file:\n",
    "    positive_words = set([line.strip() for line in pos_file])\n",
    "    \n",
    "negative_words_file = r\"negative-words.txt\"\n",
    "with open(negative_words_file, 'r', encoding='latin-1') as neg_file:\n",
    "    negative_words = set([line.strip() for line in neg_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e1d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove stop words from a text\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d23039e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the positive score using the positive word dictionary\n",
    "def calculate_positive_score(text, positive_words):\n",
    "    words = text.split()\n",
    "    positive_score = sum(1 if word.lower() in positive_words else 0 for word in words)\n",
    "    return positive_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "463b581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the negative score using the negative word dictionary\n",
    "def calculate_negative_score(text, negative_words):\n",
    "    words = text.split()\n",
    "    negative_score = sum(-1 if word.lower() in negative_words else 0 for word in words)\n",
    "    return negative_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab79f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the polarity score\n",
    "def calculate_polarity_score(positive_score, negative_score):\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    return polarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77e04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the subjectivity score\n",
    "def calculate_subjectivity_score(positive_score, negative_score, total_words):\n",
    "    subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
    "    return subjectivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c96542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the count of complex words\n",
    "def calculate_complex_word_count(text):\n",
    "    words = text.split()\n",
    "    complex_word_count = sum(1 for word in words if syllables.estimate(word) > 2)\n",
    "    return complex_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3de1198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    # Convert the word to lowercase for consistent counting\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Remove common suffixes that do not contribute to syllable count\n",
    "    exceptions = [\"es\", \"ed\"]\n",
    "    for exception in exceptions:\n",
    "        if word.endswith(exception):\n",
    "            word = word[:-len(exception)]\n",
    "    \n",
    "    # Count the number of vowels (a, e, i, o, u) in the word\n",
    "    vowels = \"aeiou\"\n",
    "    syllable_count = sum(1 for letter in word if letter in vowels)\n",
    "    \n",
    "    # Handle words with no vowels\n",
    "    if syllable_count == 0:\n",
    "        syllable_count = 1\n",
    "    \n",
    "    return syllable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8ffedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_personal_pronouns(text):\n",
    "    # Define the regular expression pattern for personal pronouns\n",
    "    pronoun_pattern = r'\\b(?:[Ii]|we|my|ours|us)\\b'\n",
    "    \n",
    "    # Use the regex pattern to find and count personal pronouns in the text\n",
    "    personal_pronoun_matches = re.findall(pronoun_pattern, text)\n",
    "    \n",
    "    # Exclude instances where \"US\" refers to the country name\n",
    "    filtered_pronouns = [pronoun for pronoun in personal_pronoun_matches if pronoun.lower() != 'us']\n",
    "    \n",
    "    return len(filtered_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717d2434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the average word length\n",
    "def calculate_average_word_length(text):\n",
    "    words = word_tokenize(text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    total_words = len(words)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if total_words > 0:\n",
    "        avg_word_length = total_characters / total_words\n",
    "    else:\n",
    "        avg_word_length = 0\n",
    "    \n",
    "    return avg_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f35a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to analyze a text file and calculate the specified factors\n",
    "def analyze_text_file(file_path,sia):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "        # Remove stop words\n",
    "        cleaned_text = remove_stopwords(text)\n",
    "\n",
    "        # Calculate sentiment scores\n",
    "        sentiment_scores = sia.polarity_scores(cleaned_text)\n",
    "        positive_score = calculate_positive_score(cleaned_text,positive_words)\n",
    "        negative_score = calculate_negative_score(cleaned_text,negative_words)\n",
    "        \n",
    "        polarity_score = calculate_polarity_score(positive_score, negative_score)\n",
    "        \n",
    "        # Tokenize the cleaned text into sentences and words\n",
    "        sentences = sent_tokenize(cleaned_text)\n",
    "        words = word_tokenize(cleaned_text)\n",
    "        \n",
    "        subjectivity_score = calculate_subjectivity_score(positive_score, negative_score, len(words))\n",
    "        \n",
    "        # Calculate Average Sentence Length\n",
    "        avg_sentence_length = len(words) / len(sentences) if len(sentences) > 0 else 0\n",
    "        \n",
    "        #Calculate complex word count\n",
    "        complex_word_count = calculate_complex_word_count(cleaned_text)\n",
    "        \n",
    "        # Calculate Percentage of Complex Words\n",
    "        percentage_complex_words = complex_word_count / len(words) if len(words) > 0 else 0\n",
    "        \n",
    "        # Calculate Fog Index\n",
    "        fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "        \n",
    "        # Calculate Average Number of Words Per Sentence\n",
    "        avg_words_per_sentence = len(words) / len(sentences) if len(sentences) > 0 else 0\n",
    "        \n",
    "        #Calculate the word count\n",
    "        word_count = len(words)\n",
    "        \n",
    "        # Calculate Syllables Per Word\n",
    "        syllables_per_word = sum(count_syllables(word) for word in words) / len(words) if len(words) > 0 else 0\n",
    "        \n",
    "        # Calculate Personal Pronoun Count\n",
    "        personal_pronoun_count = count_personal_pronouns(cleaned_text)\n",
    "        \n",
    "        # Calculate Average Word Length\n",
    "        average_word_length = calculate_average_word_length(cleaned_text)\n",
    "        \n",
    "        factors = {\n",
    "            'POSITIVE SCORE': positive_score,\n",
    "            'NEGATIVE SCORE': negative_score,\n",
    "            'POLARITY SCORE': polarity_score,\n",
    "            'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "            'AVG SENTENCE LENGTH': avg_sentence_length,\n",
    "            'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
    "            'FOG INDEX': fog_index,\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,\n",
    "            'COMPLEX WORD COUNT': complex_word_count,\n",
    "            'WORD COUNT': word_count,\n",
    "            'SYLLABLES PER WORD': syllables_per_word,\n",
    "            'PERSONAL PRONOUNS': personal_pronoun_count,\n",
    "            'AVERAGE WORD LENGTH': average_word_length  \n",
    "        }\n",
    "        \n",
    "        return factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a076a06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded successfully!\n",
      "DataFrame columns: Index(['URL_ID', 'URL'], dtype='object')\n",
      "       URL_ID                                                URL  \\\n",
      "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
      "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
      "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
      "3  bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
      "4  bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
      "\n",
      "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
      "0              38              -8        1.533333            0.051282   \n",
      "1               6               0        1.000000            0.033149   \n",
      "2               7               0        1.000000            0.038674   \n",
      "3               6               0        1.000000            0.032967   \n",
      "4               7               0        1.000000            0.038462   \n",
      "\n",
      "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS   FOG INDEX  \\\n",
      "0                585.0                     0.569231  234.227692   \n",
      "1                181.0                     0.508287   72.603315   \n",
      "2                181.0                     0.535912   72.614365   \n",
      "3                182.0                     0.527473   73.010989   \n",
      "4                182.0                     0.532967   73.013187   \n",
      "\n",
      "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
      "0                             585.0                 333         585   \n",
      "1                             181.0                  92         181   \n",
      "2                             181.0                  97         181   \n",
      "3                             182.0                  96         182   \n",
      "4                             182.0                  97         182   \n",
      "\n",
      "   SYLLABLES PER WORD  PERSONAL PRONOUNS  AVERAGE WORD LENGTH  \n",
      "0            2.986325                  0             8.247863  \n",
      "1            3.071823                  0             8.585635  \n",
      "2            3.093923                  0             8.618785  \n",
      "3            3.104396                  0             8.576923  \n",
      "4            3.104396                  0             8.626374  \n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the text files\n",
    "text_files_directory = r\"G:\\20211030 Test Assignment\\blackcoffer\"\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Load the DataFrame 'df' and ensure it has 'URL_ID' as string\n",
    "file_path = 'Input.xlsx'\n",
    "try:\n",
    "    df = pd.read_excel(file_path, dtype={'URL_ID': str})  # Ensure URL_ID is read as a string\n",
    "    print(\"DataFrame loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading DataFrame: {e}\")\n",
    "    df = pd.DataFrame()  # Create an empty DataFrame in case of error\n",
    "\n",
    "# Check DataFrame structure\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "\n",
    "# Iterate through text files and analyze each one\n",
    "for filename in os.listdir(text_files_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(text_files_directory, filename)\n",
    "        url_id = filename.split('.txt')[0]\n",
    "\n",
    "        # Ensure URL_ID is a string\n",
    "        url_id = str(url_id)\n",
    "\n",
    "        # Find the URL associated with the URL_ID by iterating through the DataFrame\n",
    "        url = None\n",
    "        for index, row in df.iterrows():\n",
    "            if row['URL_ID'] == url_id:\n",
    "                url = row['URL']\n",
    "                break  # Found the URL, so exit the loop\n",
    "\n",
    "        if url is None:\n",
    "            print(f\"URL not found for URL_ID: {url_id}\")\n",
    "            continue\n",
    "\n",
    "        # Create a dictionary with the URL_ID, URL, and factors\n",
    "        factors = analyze_text_file(file_path, sia)  # Ensure this function is defined\n",
    "        row_data = {'URL_ID': url_id, 'URL': url, **factors}\n",
    "        data.append(row_data)\n",
    "\n",
    "# Create a DataFrame from the list of data\n",
    "output = pd.DataFrame(data)\n",
    "print(output.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac036bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output Excel file path\n",
    "output_excel_file = \"output_analysis.xlsx\"\n",
    "# Save the DataFrame to an Excel file\n",
    "output.to_excel(output_excel_file, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a02ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_positive_score(text, positive_words):\n",
    "    \"\"\"\n",
    "    Calculate the positive score of the text based on the positive words dictionary.\n",
    "    \n",
    "    Args:\n",
    "    - text: The input text for which to calculate the positive score.\n",
    "    - positive_words: A set of words considered as positive.\n",
    "    \n",
    "    Returns:\n",
    "    - positive_score: The total positive score for the input text.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split the text into individual words\n",
    "    positive_score = sum(1 for word in words if word.lower() in positive_words)  # Count positive words\n",
    "    return positive_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0c56fae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positive_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositive Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mpositive_score\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'positive_score' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Positive Score:\", positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83865985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
